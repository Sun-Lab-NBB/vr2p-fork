import re
import shutil
from pathlib import Path

from tqdm import tqdm
import zarr
import numpy as np
from natsort import natsorted
import numcodecs
from suite2p.extraction import dcnv

from vr2p.signal import demix_traces
from vr2p.gimbl.parse import parse_gimbl_log


class LogFile:
    """THis is kind of a hacky way to get the log DataFrame stored in zarr
    """
    def __init__(self,value):
        self.value = value

def process_session_data(data_info, settings):
    """Processes and aggregates multiday data based on settings (read from yml fil)

    # HDF5 organization
    ## Signal data.
        * single/(F, Fdemix, Fneu, Fns, and spks)/0 # Single session data (not aligned)
        * multi/(F, Fdemix, Fneu, Fns, and spks)/0  # Multi session aligned data.
    ## Cells masks.
        * cells/single/0              # Pickled cell data of original sessions (not aligned)
        * cells/multi/original/0      # Pickled cell data of multi-session cells masks transformed to original spatial coordinates.
        * cells/multi/registered/0    # Pickled cell data of multi-session cells masks in registered spatial coordinates.
    ## Images.
        * images/original/0           # Original session images.
        * images/registered/0         # Registered session images.
    ## VR Info.
        * gimbl/log/0                 # Pickled raw pandas vr log.
        * gimbl/vr/0                  # Pickled processed vr data.

    Args:
        settings ([dict]): Settings generated by parse_settings
    """
    # load info.
    multiday_folder = Path(data_info["data"]["local_processed_root"])/data_info["data"]["output_folder"]
    info = np.load(multiday_folder/"info.npy",allow_pickle=True).item()
    # Create output dir
    save_folder = multiday_folder
    save_folder.mkdir(parents=True, exist_ok=True)
    # get imaging info.
    ops_file = multiday_folder/"sessions"/info["data_paths"][0]/"ops.npy"
    ops = np.load(ops_file,allow_pickle=True).item()
    settings["imaging"] = {"frame_rate":ops["fs"],"num_planes":ops["nplanes"]}
    # copy some settings
    for field in ["fs","Lx","Ly"]:
        settings["demix"][field] = ops[field]
    settings["animal"] = data_info["animal"]
    settings["individual_sessions"] = data_info["data"]["individual_sessions"]

    # Process single session suite2p data.
    zarr_folder = save_folder/"vr2p.zarr"
    if zarr_folder.is_dir():
        print(f"Removing previous data at {zarr_folder}")
        shutil.rmtree(zarr_folder, ignore_errors=True)
    with zarr.open(zarr_folder.as_posix(), mode="w") as f:
        # load info.
        backwards_deformed = np.load(multiday_folder/"backwards_deformed_cell_masks.npy",allow_pickle=True)
        cell_templates = np.load(multiday_folder/"cell_templates.npy",allow_pickle=True)
        trans_images = np.load(multiday_folder/"trans_images.npy",allow_pickle=True)
        original_images = np.load(multiday_folder/"original_images.npy",allow_pickle=True)
        # store settings.
        f.create_dataset("meta", data= settings, dtype=object, object_codec = numcodecs.Pickle())
        # store registered mask templates
        f.require_group("cells/multi").create_dataset("registered", data= cell_templates, dtype=object, object_codec = numcodecs.Pickle())
        def add_imaging_data(folder_path,group,stat,selected_cells=None):
            # load fluorescence data.
            if selected_cells:
                F = np.load(folder_path/"F.npy")[selected_cells,:]
                Fneu = np.load(folder_path/"Fneu.npy")[selected_cells,:]
            else:
                F = np.load(folder_path/"F.npy")
                Fneu = np.load(folder_path/"Fneu.npy")
            f.require_group(f"{group}/F").create_dataset(str(counter), data=F, chunks=(1000,10000))
            f.require_group(f"{group}/Fneu").create_dataset(str(counter), data=Fneu, chunks=(1000,10000))
            # neuropil subtraction
            Fns = F - settings["demix"]["neucoeff"] * Fneu
            f.require_group(f"{group}/Fns").create_dataset(str(counter), data=Fns, chunks=(1000,10000))
            # demix. (demixed,neuropil, and baseline subtracted signal.)
            Fdemix, _, _, _ = demix_traces(F, Fneu, stat, settings["demix"])
            f.require_group(f"{group}/Fdemix").create_dataset(str(counter), data=Fdemix, chunks=(1000,10000))
            #spks.
            spks = dcnv.oasis(Fdemix, ops["batch_size"],ops["tau"],ops["fs"])
            f.require_group(f"{group}/spks").create_dataset(str(counter), data=spks, chunks=(1000,10000))

        # process and add on individual sessions to include (will lack multi_session fluorescence data.)
        data_paths = info["data_paths"].copy()
        if data_info["data"]["individual_sessions"]:
            for date in data_info["data"]["individual_sessions"]:
                # check its in right format.
                if not re.match(r"[0-9][0-9][0-9][0-9]_[0-9][0-9]_[0-9][0-9]/[0-9]", date):
                    raise NameError(f"Requested individual_session: {date}, did not match the expected format (e.g. 2020_01_01/1)")
                # check not already in myltiday aligned list.
                if date in data_paths:
                    raise NameError(f"{date} was requested to be added as an individual (non-aligned) session but was part of the multiday registration process.")
        data_paths += data_info["data"]["individual_sessions"]
        data_paths = natsorted(data_paths)
        # store dates.
        f.create_dataset("data_paths", data= data_paths)

        for counter, data_path in tqdm(enumerate(data_paths),desc="Processing session",total=len(data_paths)):
            multi_index = np.argwhere(np.array(info["data_paths"])==data_path).squeeze()# location this data_path in multi_session structure (empty if individual sessions)
            ###
            # VR info
            ###
            log_file = find_gimbl_log(Path(data_info["data"]["local_processed_root"])/data_path)
            df, vr_info = parse_gimbl_log(log_file)
            f.require_group("gimbl/vr").create_dataset(str(counter), data=vr_info, dtype=object, object_codec = numcodecs.Pickle())
            f.require_group("gimbl/log").create_dataset(str(counter), data=LogFile(df), dtype=object, object_codec = numcodecs.Pickle()) # store as own class since I had realt troubles loading it back in otherwise

            ###
            # Single session
            ###
            session_path = Path(data_info["data"]["local_processed_root"])/data_path/data_info["data"]["suite2p_folder"]/"combined"
            # read cell info.
            stat = np.load(session_path/"stat.npy",allow_pickle=True)
            iscell = np.load(session_path/"iscell.npy",allow_pickle=True)
            # select valid cells and only needed fields.
            selected_cells =  [ (iscell[icell,1]>settings["cell_detection"]["prob_threshold"]) and (mask["npix"]<settings["cell_detection"]["max_size"]) for icell, mask in enumerate(stat)]
            # store cell info.
            f.require_group("cells/single").create_dataset(str(counter), data= stat[selected_cells], dtype=object, object_codec = numcodecs.Pickle())
            # fluorescence data.
            add_imaging_data(session_path,"single",stat[selected_cells],selected_cells)

            ###
            # Multiple sessions
            ###
            if multi_index.size!=0:
                session_path = multiday_folder/"sessions"/data_path
                # read individual session cell info.
                stat = backwards_deformed[multi_index]

                # Fluorescence data.
                add_imaging_data(session_path,"multi",stat)

                # Cells.
                f.require_group("cells/multi/original").create_dataset(str(counter), data=stat, dtype=object, object_codec = numcodecs.Pickle())
                f.require_group("images/registered").create_dataset(str(counter), data=trans_images[multi_index], dtype=object, object_codec = numcodecs.Pickle())
                # Images.
                f.require_group("images/original").create_dataset(str(counter), data= original_images[multi_index], dtype=object, object_codec = numcodecs.Pickle())
            # Session is added individually so theres no multi_session info.
            else:
                # empty cell info
                empty_stat = {}
                for key in stat[0].keys():
                    empty_stat[key] = None
                f.require_group("cells/multi/original").create_dataset(str(counter), data=[empty_stat], dtype=object, object_codec = numcodecs.Pickle())
                # empty FLuorescence data.
                num_frames = vr_info.position.frame.reset_index()["frame"].max()
                num_cells = len(cell_templates)
                empty_array = np.empty((num_cells,num_frames))
                empty_array[:] = np.nan
                for field in ["F","Fneu","Fns","Fdemix","spks"]:
                    f.require_group(f"multi/{field}").create_dataset(str(counter), data=empty_array, chunks=(1000,10000))
                # get original images.
                ops = np.load(session_path/"ops.npy",allow_pickle=True).item()
                imgs = {"mean_img":ops["meanImg"], "enhanced_img":  ops["meanImgE"], "max_img": ops["max_proj"]}
                f.require_group("images/original").create_dataset(str(counter), data= imgs, dtype=object, object_codec = numcodecs.Pickle())
                # empty registered images.
                empty_img = np.zeros(ops["meanImg"].shape)
                empty_imgs = {}
                for key in ["mean_img", "enhanced_img", "max_img"]:
                    empty_imgs[key] = empty_img
                f.require_group("images/registered").create_dataset(str(counter), data= empty_imgs, dtype=object, object_codec = numcodecs.Pickle())

def find_gimbl_log(folder_path):
    """Find Gimbl log json file in folder. Note: assumes word Log/log is in title.

    Args:
        folder_path (Path): Folder where to look for log
    """
    log_file = list(folder_path.glob("*Log*.json"))
    if not log_file:
        raise NameError(f"Could not find Gimbl Log in {folder_path}\nDoes file name include 'L(l)og' in its name?")
    if len(log_file)>1:
        raise NameError(f"Found multiple possible Gimbl log files in {folder_path}\nDo multiple json files include 'L(l)og' in their name?")
    log_file= log_file[0]
    return log_file

